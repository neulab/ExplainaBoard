# SummEval Dataset

## Description
**SummEval** is a collection of human
judgments of model-generated summaries on the subset (100 samples) of CNNDM annotated by both expert judges and
crowd-source workers. Each system generated summary is gauged through the lens of *coherence*, *consistency*, *fluency* and *relevance*.


## Meta Data
* Github Repo: [Yale-LILY/SummEval](https://github.com/Yale-LILY/SummEval)
* Paper: [SummEval: Re-evaluating Summarization Evaluation](https://arxiv.org/abs/2007.12626)
* Aspect: coherence, consistency, fluency, relevance
* Language: English


## Data Structure
### Example
We collate the human judgements inside `data.jsonl`. A data sample is shown in the following json format.

```
{
    'src': "This is the source text",
    'ref_summs': [
                    "This is the reference summary1", 
                    "This is the reference summary2",
                    ...
                ],
    'ref_summ': "This is the first one in ref_summs"
    'sys_summs': {
        'M11': {
            'sys_summ': "This is the summary generated by system M11.",
            'scores': {
                        'coherence': 1.3333333333333333,
                        'consistency': 1.0,
                        'fluency': 3.0,
                        'relevance': 1.6666666666666667
                       }
                },
        'M13': {
            'sys_summ': "This is the summary generated by system M13.",
            'scores': {
                        'coherence': 2.3333333333333335,
                        'consistency': 5.0,
                        'fluency': 5.0,
                        'relevance': 2.6666666666666665
                       }
                },
        ...
    },
}
```

### Format
Each data sample contains the following fields:
* `src`: The tokenized, normal-cased source text.
* `ref_summs`: Multiple  tokenized, normal-cased reference summaries
* `ref_summ`: The first one in `ref_summs`.
* `sys_summs`: Collected system summaries together with their human judgement scores.
    * `sys_summ`: The tokenized, normal-cased system generated summary.
    * `scores`: For `data.jsonl`, there are only human judgement scores stored as key-value dictionary here. For system output files, the automatic metric scores will also be displayed here (e.g. `{"auto-metric1": 0.2}`).


## Reference
```
@misc{fabbri2021summeval,
      title={SummEval: Re-evaluating Summarization Evaluation}, 
      author={Alexander R. Fabbri and Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher and Dragomir Radev},
      year={2021},
      eprint={2007.12626},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```